{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDSC INTRODUCTION TO AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover turorial lessons and then exercises to learn basic AI concepts and popular algorithms. \n",
    "\n",
    "Let's begin with a brief AI recap.\n",
    "\n",
    "What's AI about: Systems/machines/robots mimicking human intelligence\n",
    "\n",
    "Example:\n",
    "Customer service chatbots\n",
    "Recommender Systems\n",
    "Self driving cars\n",
    "\n",
    "<b>Fields in AI </b>\n",
    "\n",
    "<b>Machine Learning:</b> computers the potential to learn without being programmed\n",
    "<b>Neural Networks:</b> imitates operation of human brain\n",
    "<b>Deep Learning:</b> selects “best” of several methods to produce desirable output\n",
    "<b>Cognitive Processing:</b> complex task completion or problem solving between man and machine\n",
    "<b>Computer Vision:</b> machines interpret visual data\n",
    "<b>Natural Language Processing:</b> communication between human and machine using natural language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Algorithms</b>\n",
    "- Regression\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Simple NeuraL Networks\n",
    "- Convolutional Neural Networks\n",
    "\n",
    "These are some algorithms, useful depending on the kind of problem you are trying to solve. \n",
    "\n",
    "For the purposes of this lesson we will practice building Machine Learning models and just mention the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MACHINE LEARNING</b>\n",
    "\n",
    "What is machine learning though? How can we make a computer make decisions on its own without having to instruct \n",
    "it every single time it encouters a problem. That's the question machine learning seeks to answer. It is a combination of statistics/modelling and computer science concepts to help machines learn and from situations and come up with solutions without assistance. \n",
    "\n",
    "there are 3 popular ways of teaching machines, in other words, what is the best way for a machine to learn most effectively in order to solve an existing problem.\n",
    "- Supervised Learning: \n",
    "    Labelled data is provided to our machine, it studies this data very well such that when it is given new set of unlabelled data, it understands it and is able to make a decision on it based on what it learned. Used in problimes like weathe forcasting, spam detection, pricing prediction. \n",
    "    Types - Regression, classification, support vector machines, random forests\n",
    "- Unsupervised Learning:the machine learns from unlabelled data. It uses methods like clustering or association to learn the data and then use this knowledge to inform decisions about new unlabelled data. Useful in solving problems such as anomaly detection, medical imaging etc\n",
    "   Types: Association, Classification \n",
    "- Reinforcement Learning: The machines acts and gets rewards for it's action.depending on the reward it will know if that action should be carried out in that situation or not. Very widely used in solving problems such as self driving cars. It is very widely used in multiple areas. \n",
    "\n",
    "We will look at some types of supervised learning.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SUPERVISED LEARNING</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Linear Regression</b>\n",
    "\n",
    " We find the relation between continous variables in dataset.  Here, the objective is to find the best fit line\n",
    "    that will accurately make a prediction. We have a set of independent continous variables and a dependent \n",
    "    numeric variable. \n",
    " \n",
    " <b>Steps in Linear Regression</b>\n",
    " \n",
    " 1. Load the data set\n",
    " 2. Build Linear Regression Model\n",
    " 3. Fit the model\n",
    " 4. Make predictions\n",
    " 5. Check how well you model made the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this linear regression tutorial we will work with data from sklearn so you need to be connected to \n",
    "the internet to load the data into the notebook.\n",
    "\n",
    "sklearn has multiple amazing data sets you can use for practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Boston Housing Data</b>\n",
    "\n",
    "In this tutorial we will use linear regression to predict the prices of a house with knowledge of the features \n",
    "of the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression #linear model from sklearn\n",
    "from sklearn.model_selection import train_test_split #help us split data to dependent and independent variables\n",
    "\n",
    "#load dataset\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_dataset = load_boston()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just for the purposes of viewing the dataset\n",
    "from pandas import *\n",
    "df = DataFrame(boston_dataset.data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = boston_dataset.target #dependent variable \n",
    "X = boston_dataset.data #independent continous variables/features\n",
    "\n",
    "#split the data into train and test set, use 20% of our dataset for dataset and the remaining 80% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Build Linear Regression Model\n",
    "#create instance of regression model\n",
    "linr_model = LinearRegression()\n",
    "\n",
    "#fit the model using the training data\n",
    "linr_model.fit(X_train, y_train)\n",
    "\n",
    "#Predict using the model\n",
    "linr_predict = linr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#save ml models in csv files and compare their performance\n",
    "import joblib \n",
    "\n",
    "admissions_dataset = pd.read_csv(\"/home/kuna/Downloads/bertelsmann/graduate-admissions/Admission_Predict.csv\")# Creating input feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "df = DataFrame(admissions_dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features\n",
    "X= admissions_dataset[['GRE Score','TOEFL Score','University Rating','SOP','LOR ', 'CGPA']]\n",
    "# Creating target variable\n",
    "y= admissions_dataset [['Chance of Admit ']] \n",
    "\n",
    "#Creating Train and Test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature Scaling</b>\n",
    "Feature has to do with transforming a dataset into a common range of values. This can be done by either [Normalizing](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/) or [Standardizing](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/) columns in the dataset.\n",
    "\n",
    "Feature scaling is most useful when dealing with ML algorithms that use distance based metrics. We may talk about it when working with SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StandardScalar a library for normalizing the input features and independent variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stanc_x= StandardScaler()\n",
    "X_train = stanc_x.fit_transform(X_train)\n",
    "X_test = stanc_x.transform(X_test)\n",
    "\n",
    "#Normalizing the target variable\n",
    "stanc_y= StandardScaler()\n",
    "y_train = stanc_y.fit_transform(y_train)\n",
    "y_test = stanc_y.transform(y_test)\n",
    "\n",
    "#Build Linear Regression Model\n",
    "#create instance of regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg_model = LinearRegression()# fitting the training data to the Linear Regressor\n",
    "linreg_model.fit(X_train,y_train)# Checking the model coefficients\n",
    "\n",
    "#Predict using the model\n",
    "y_predict = linreg_model.predict(x_test)\n",
    "\n",
    "linreg_model.score(X_test, y_test) #print accuracy of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use joblib to create a file for the predicted model and compare it with that of the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a joblib file for the model\n",
    "joblib.dump(linreg_model, '/home/kuna/Downloads/bertelsmann/joblib_test.sav') \n",
    " \n",
    "# Load the jobllib file\n",
    "linreg_model_load = joblib.load('/home/kuna/Downloads/bertelsmann/joblib_test.sav') \n",
    " \n",
    "# Check that the loaded model is the same as the original\n",
    "assert linreg_model.score(x, y) == linreg_model_load.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [assert](https://doc.dataiku.com/dss/latest/machine-learning/ml-assertions.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Logistic Regression</b>\n",
    "\n",
    " Used for classification probems. Output is of form Yes/No, 1/0 etc. We classify items in a data set. Here, the line can be a curve or take any shape, must not necessarily be a straight line so not useful describe linear relationships. We have a set of independent categorical variables and a dependent variable. \n",
    " Uses an activation function known as the sigmoid function to convert a linear regression equation to the logistic regression equation.\n",
    " \n",
    " <b>Steps in Logistic Regression</b>\n",
    " \n",
    " 1. Load the data set\n",
    " 2. Build Logistic Regression Model\n",
    " 3. Predict using the model\n",
    " 4. Check how well you model made the prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Iris Data</b>\n",
    "\n",
    "Here, we classify the flowers in the iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression #linear model from sklearn\n",
    "from sklearn.model_selection import train_test_split #help us split data to dependent and independent variables\n",
    "\n",
    "#load dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_dataset = load_iris() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just for the purposes of viewing the dataset\n",
    "from pandas import *\n",
    "dfi = DataFrame(iris_dataset.data)\n",
    "#dfi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuna/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "y = iris_dataset.target \n",
    "X = iris_dataset.data \n",
    "\n",
    "#split the data into train and test set, use 20% of our dataset for dataset and the remaining 80% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Build Logistic Regression Model\n",
    "#create instance of model\n",
    "logr_model = LogisticRegression()\n",
    "\n",
    "#fit the model using the training data\n",
    "logr_model.fit(X_train, y_train)\n",
    "\n",
    "#Predict using the model\n",
    "logr_predict = logr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer [here](http://localhost:8888/notebooks/Downloads/bertelsmann/logistic-regression-classifier-tutorial%20from%20Kaggle.ipynb) for example on Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>IGNORE THE REST OF THE NOTES</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix identify % prediction that did not work well\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predicted)\n",
    "cm\n",
    "\n",
    "#prints the plot of predicted against true \n",
    "import seaborn as sn\n",
    "plt.figure(figsize = (10, 7))\n",
    "sn.heatmap(cm, annot =True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Handling Missing values with Imputer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Imputer' from 'sklearn.preprocessing' (/home/kuna/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-a4618f992549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'NaN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Imputer' from 'sklearn.preprocessing' (/home/kuna/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
